{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99d6e8e3-e7d9-458e-a9e3-c64e967a510e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.45.2 in /opt/conda/lib/python3.10/site-packages (4.45.2)\n",
      "Requirement already satisfied: optimum==1.23.2 in /opt/conda/lib/python3.10/site-packages (from optimum[exporters,onnxruntime]==1.23.2) (1.23.2)\n",
      "Requirement already satisfied: accelerate>=0.34.0 in /opt/conda/lib/python3.10/site-packages (1.10.1)\n",
      "Requirement already satisfied: onnx in /opt/conda/lib/python3.10/site-packages (1.19.0)\n",
      "Requirement already satisfied: onnxruntime in /opt/conda/lib/python3.10/site-packages (1.23.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.2) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.2) (0.35.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.2) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.2) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.2) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.2) (2025.9.18)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.2) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.2) (0.6.2)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.2) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.2) (4.66.5)\n",
      "Requirement already satisfied: coloredlogs in /opt/conda/lib/python3.10/site-packages (from optimum==1.23.2->optimum[exporters,onnxruntime]==1.23.2) (15.0.1)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from optimum==1.23.2->optimum[exporters,onnxruntime]==1.23.2) (1.14.0)\n",
      "Requirement already satisfied: torch>=1.11 in /opt/conda/lib/python3.10/site-packages (from optimum==1.23.2->optimum[exporters,onnxruntime]==1.23.2) (2.8.0)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from optimum==1.23.2->optimum[exporters,onnxruntime]==1.23.2) (4.2.0)\n",
      "Requirement already satisfied: timm in /opt/conda/lib/python3.10/site-packages (from optimum[exporters,onnxruntime]==1.23.2) (1.0.20)\n",
      "Requirement already satisfied: evaluate in /opt/conda/lib/python3.10/site-packages (from optimum[exporters,onnxruntime]==1.23.2) (0.4.6)\n",
      "Requirement already satisfied: protobuf>=3.20.1 in /opt/conda/lib/python3.10/site-packages (from optimum[exporters,onnxruntime]==1.23.2) (4.25.5)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.34.0) (6.0.0)\n",
      "Requirement already satisfied: typing_extensions>=4.7.1 in /opt/conda/lib/python3.10/site-packages (from onnx) (4.12.2)\n",
      "Requirement already satisfied: ml_dtypes in /opt/conda/lib/python3.10/site-packages (from onnx) (0.5.3)\n",
      "Requirement already satisfied: flatbuffers in /opt/conda/lib/python3.10/site-packages (from onnxruntime) (25.9.23)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum==1.23.2->optimum[exporters,onnxruntime]==1.23.2) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum==1.23.2->optimum[exporters,onnxruntime]==1.23.2) (0.3.9)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->optimum==1.23.2->optimum[exporters,onnxruntime]==1.23.2) (2.2.1)\n",
      "Requirement already satisfied: httpx<1.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum==1.23.2->optimum[exporters,onnxruntime]==1.23.2) (0.27.2)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->optimum==1.23.2->optimum[exporters,onnxruntime]==1.23.2) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum==1.23.2->optimum[exporters,onnxruntime]==1.23.2) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets->optimum==1.23.2->optimum[exporters,onnxruntime]==1.23.2) (2024.9.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.2) (1.1.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.45.2) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.45.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.45.2) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.45.2) (2024.8.30)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum==1.23.2->optimum[exporters,onnxruntime]==1.23.2) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum==1.23.2->optimum[exporters,onnxruntime]==1.23.2) (3.1.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum==1.23.2->optimum[exporters,onnxruntime]==1.23.2) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum==1.23.2->optimum[exporters,onnxruntime]==1.23.2) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum==1.23.2->optimum[exporters,onnxruntime]==1.23.2) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum==1.23.2->optimum[exporters,onnxruntime]==1.23.2) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum==1.23.2->optimum[exporters,onnxruntime]==1.23.2) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum==1.23.2->optimum[exporters,onnxruntime]==1.23.2) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum==1.23.2->optimum[exporters,onnxruntime]==1.23.2) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum==1.23.2->optimum[exporters,onnxruntime]==1.23.2) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum==1.23.2->optimum[exporters,onnxruntime]==1.23.2) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum==1.23.2->optimum[exporters,onnxruntime]==1.23.2) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum==1.23.2->optimum[exporters,onnxruntime]==1.23.2) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum==1.23.2->optimum[exporters,onnxruntime]==1.23.2) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum==1.23.2->optimum[exporters,onnxruntime]==1.23.2) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum==1.23.2->optimum[exporters,onnxruntime]==1.23.2) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11->optimum==1.23.2->optimum[exporters,onnxruntime]==1.23.2) (3.4.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /opt/conda/lib/python3.10/site-packages (from triton==3.4.0->torch>=1.11->optimum==1.23.2->optimum[exporters,onnxruntime]==1.23.2) (75.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->optimum==1.23.2->optimum[exporters,onnxruntime]==1.23.2) (1.3.0)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]>=4.29->optimum==1.23.2->optimum[exporters,onnxruntime]==1.23.2) (0.2.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /opt/conda/lib/python3.10/site-packages (from coloredlogs->optimum==1.23.2->optimum[exporters,onnxruntime]==1.23.2) (10.0)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from timm->optimum[exporters,onnxruntime]==1.23.2) (0.23.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets->optimum==1.23.2->optimum[exporters,onnxruntime]==1.23.2) (3.10.11)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx<1.0.0->datasets->optimum==1.23.2->optimum[exporters,onnxruntime]==1.23.2) (4.6.0)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1.0.0->datasets->optimum==1.23.2->optimum[exporters,onnxruntime]==1.23.2) (1.0.6)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx<1.0.0->datasets->optimum==1.23.2->optimum[exporters,onnxruntime]==1.23.2) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1.0.0->datasets->optimum==1.23.2->optimum[exporters,onnxruntime]==1.23.2) (0.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11->optimum==1.23.2->optimum[exporters,onnxruntime]==1.23.2) (3.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum==1.23.2->optimum[exporters,onnxruntime]==1.23.2) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum==1.23.2->optimum[exporters,onnxruntime]==1.23.2) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum==1.23.2->optimum[exporters,onnxruntime]==1.23.2) (2024.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->timm->optimum[exporters,onnxruntime]==1.23.2) (10.4.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->optimum==1.23.2->optimum[exporters,onnxruntime]==1.23.2) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->optimum==1.23.2->optimum[exporters,onnxruntime]==1.23.2) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->optimum==1.23.2->optimum[exporters,onnxruntime]==1.23.2) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->optimum==1.23.2->optimum[exporters,onnxruntime]==1.23.2) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->optimum==1.23.2->optimum[exporters,onnxruntime]==1.23.2) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->optimum==1.23.2->optimum[exporters,onnxruntime]==1.23.2) (1.14.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->optimum==1.23.2->optimum[exporters,onnxruntime]==1.23.2) (4.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->optimum==1.23.2->optimum[exporters,onnxruntime]==1.23.2) (1.16.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx<1.0.0->datasets->optimum==1.23.2->optimum[exporters,onnxruntime]==1.23.2) (1.2.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets->optimum==1.23.2->optimum[exporters,onnxruntime]==1.23.2) (0.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#%pip uninstall -y optimum-onnx\n",
    "# (Optional) clean up prior mismatches\n",
    "#%pip uninstall -y optimum transformers tokenizers accelerate\n",
    "# Reinstall a known-good set\n",
    "%pip install --no-cache-dir \\\n",
    "  \"transformers==4.45.2\" \\\n",
    "  \"optimum[exporters,onnxruntime]==1.23.2\" \\\n",
    "  \"accelerate>=0.34.0\" \\\n",
    "  onnx onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8d6159f-2be3-40f9-ae87-2d8acc19da6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import mlflow\n",
    "import requests\n",
    "from mlflow.exceptions import MlflowException\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.store.artifact.runs_artifact_repo import RunsArtifactRepository\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from optimum.exporters.tasks import TasksManager\n",
    "from optimum.exporters.onnx import export\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a335f573-570e-4339-b443-1937b08aeda1",
   "metadata": {},
   "source": [
    "### Download models from hugging face\n",
    "In this step we download two models from Hugging Face\n",
    "1. Bert-based\n",
    "2. Distilbert-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f7ff466-b65e-4c3c-af6a-aed659553344",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cpu\"\n",
    "# ONNX opset version (14+ needed for scaled_dot_product_attention)\n",
    "OPSET = 14\n",
    "\n",
    "def download_and_export_onxx(model_name: str, output_dir: Path, task: str = \"text-classification\"):\n",
    "    if not os.path.exists(output_dir):       \n",
    "        os.makedirs(output_dir)\n",
    "    \"\"\"\n",
    "    Export a Hugging Face model to ONNX using Optimum.\n",
    "    \"\"\"\n",
    "    print(f\"Exporting {model_name} to ONNX for task={task} -> {output_dir}\")\n",
    "\n",
    "    # 1. Load model & tokenizer\n",
    "    print(\"Loading model...\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=2,  # Adjust to your actual classification labels\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # 2. Determine model_type (e.g., \"bert\", \"distilbert\")\n",
    "    model_type = model.config.model_type\n",
    "    \n",
    "\n",
    "    # 3. Build the exporter config\n",
    "    exporter_config_constructor = TasksManager.get_exporter_config_constructor(\n",
    "        model_type=model_type,\n",
    "        exporter=\"onnx\",\n",
    "        library_name=\"transformers\"  # Explicitly specify \"transformers\"\n",
    "    )\n",
    "    exporter_config = exporter_config_constructor(\n",
    "        model.config,\n",
    "        task=task\n",
    "    )\n",
    "\n",
    "    print(\"Running export...\")\n",
    "    file_path = os.path.join(output_dir,\"model.onnx\")\n",
    "    # 4. Run export\n",
    "    onnx_paths = export(\n",
    "        model=model,\n",
    "        config=exporter_config,\n",
    "        output=Path(file_path),\n",
    "        device=DEVICE,\n",
    "        opset=OPSET\n",
    "    )\n",
    "    print(\"Saving tokenizer...\")\n",
    "    tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64f74f2f-273a-4bdc-967d-1b53cdaec80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT = \"bert-base-uncased\"\n",
    "DISTILBERT = \"distilbert-base-uncased\"\n",
    "TASK = \"text-classification\"\n",
    "BERT_LOCAL_FOLDER = f\"/home/ubuntu/mymodels/{BERT}\"\n",
    "DISTILBERT_LOCAL_FOLDER = f\"/home/ubuntu/mymodels/{DISTILBERT}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "618b449d-6ea6-4bce-8b97-999f62cf2dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting bert-base-uncased to ONNX for task=text-classification -> /home/ubuntu/mymodels/bert-base-uncased\n",
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b31ebd67da0040c0bdd50fe1e047d77b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a7b1237682540fe82cfd212d6d59425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfe5ef56b782457a80cdda4445a8b94a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9d209016ac84752b8c3071b1281d91e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7ecb70e5ad6463992cd45b5a099392b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running export...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving tokenizer...\n",
      "Exporting bert-base-uncased to ONNX for task=text-classification -> /home/ubuntu/mymodels/distilbert-base-uncased\n",
      "Loading model...\n",
      "Running export...\n",
      "Saving tokenizer...\n"
     ]
    }
   ],
   "source": [
    "download_and_export_onxx(BERT, BERT_LOCAL_FOLDER,TASK)   \n",
    "download_and_export_onxx(BERT, DISTILBERT_LOCAL_FOLDER,TASK)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9a6cc3-1426-4034-bd0d-1cb5e7aa5f26",
   "metadata": {},
   "source": [
    "## Prepare to register to MLflow\n",
    "\n",
    "`EmptyTritonModel` only exists so we can register the binaries of large model. This model will not be deployed. The dataset hosting the model are stored in the folder location \n",
    "`<DATASET>/models/<registered_model_name>/<registered_model_version>`\n",
    "\n",
    "The values for the `registered_model_name` and `registered_model_version` are the values for the `EmptyTritonModel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b89eead-8b26-4f12-8af4-d417d9b29078",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmptyTritonModel(mlflow.pyfunc.PythonModel):\n",
    "    def load_context(self, context):\n",
    "        return\n",
    "    def predict(self, context, model_input, params=None):\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89793405-1c7f-4b7d-8708-c5dd49e32529",
   "metadata": {},
   "source": [
    "The `TritonModel` is the actual model which gets deployed. No model binaries are registered with it. Instead it is merely passed the following parameters in the configuration\n",
    "1. `inference_server_name` - This is the Triton Inference Server where the models are deployed.\n",
    "2. `model_name` - The `registered model name` for the associated `EmptyTritonModel`\n",
    "3. `model_name` - The `registered model version` for the associated `EmptyTritonModel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10a105b1-7a42-4d51-8f8e-10cdd547a29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TritonModel(mlflow.pyfunc.PythonModel):\n",
    "    import json\n",
    "    import requests\n",
    "    def load_context(self, context):\n",
    "        with open(context.artifacts[\"model_context\"], \"r\") as f:\n",
    "            cfg = json.load(f)\n",
    "        #self.proxy_service = cfg[\"proxy_service\"]\n",
    "        self.proxy_service = os.getenv(\"inference-proxy-service\",\"\")\n",
    "        self.inference_server_name = cfg[\"inference_server_name\"]\n",
    "        self.model_name = cfg[\"model_name\"]\n",
    "        self.model_version = cfg[\"model_version\"]\n",
    "\n",
    "    def predict(self, context, model_input, params=None):\n",
    "        print(\"Called predict\")\n",
    "        triton_model_input = {\n",
    "            \"inference_server_name\": self.inference_server_name,\n",
    "            \"model_name\": self.model_name,\n",
    "            \"model_version\": self.model_version,\n",
    "            \"payload\": model_input[\"payload\"]\n",
    "        }\n",
    "        url = f\"{self.proxy_service}/predict\"\n",
    "        if not self.proxy_service:\n",
    "            return {\n",
    "                \"status_code\": 400,\n",
    "                \"body\": \"Proxy Service Variable not set\"\n",
    "            }            \n",
    "        else:\n",
    "            resp = requests.post(url, json=triton_model_input,verify=False)\n",
    "            return {\n",
    "                \"status_code\": resp.status_code,\n",
    "                \"body\": resp.json()\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6c23db-e982-4253-bc5b-0db31c8509f0",
   "metadata": {},
   "source": [
    "MLflow code to create experiement and register model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d58a32e-4148-446e-98b7-846466b13430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_experiment(name: str) -> str:\n",
    "    # Try to get the experiment\n",
    "    experiment = mlflow.get_experiment_by_name(name)\n",
    "    if experiment:\n",
    "        return experiment.experiment_id\n",
    "\n",
    "    # Otherwise, create it\n",
    "    experiment_id = mlflow.create_experiment(name)\n",
    "    return experiment_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0b7a370-24cb-4d56-9f76-000dfd2a3827",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_registered_model(name: str):\n",
    "    client = MlflowClient()\n",
    "    try:\n",
    "        # Try to get the model details\n",
    "        model = client.get_registered_model(name)\n",
    "        print(f\"Model '{name}' already registered.\")\n",
    "    except MlflowException as e:\n",
    "        if \"RESOURCE_DOES_NOT_EXIST\" in str(e):\n",
    "            # Model doesn't exist — create it\n",
    "            model = client.create_registered_model(name)\n",
    "            print(f\"Model '{name}' created.\")\n",
    "        else:\n",
    "            raise e\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b65517-e306-4cc8-8046-524736ee33b8",
   "metadata": {},
   "source": [
    "Specify three variables:\n",
    "1. Experiment Name where the MLflow runs are registered\n",
    "2. Registered Model Name - for the `EmptyTritonModel` which stores the model binaries\n",
    "3. Client Registered Model Name - for the `TritonModel` which serves as a client model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25a2c5ca-c338-4d33-bd44-cf00e9f3a996",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name=\"TRITON-INFERENCE-SERVER-MODELS\"\n",
    "registered_model_name=\"BERT-BASED\"\n",
    "client_registered_model_name=\"BERT-BASED-CLIENT\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30c2b15f-0ce8-4ce3-9179-0eba3fa41edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 'BERT-BASED' created.\n",
      "Model 'BERT-BASED-CLIENT' created.\n"
     ]
    }
   ],
   "source": [
    "## Create Experiment and Models\n",
    "experiment_id = get_or_create_experiment(experiment_name)\n",
    "registered_model = get_or_create_registered_model(registered_model_name)\n",
    "client_registered_model = get_or_create_registered_model(client_registered_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0a367f-01c9-4616-a975-95f58bde7861",
   "metadata": {},
   "source": [
    "Convert the downloaded models to onxx format and store them to the target folder location - <DEV_DATASET>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee30f87b-c71e-478a-9419-cdcc6f8156cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TYPES = {\n",
    "    \"bert-base-uncased\": \"TYPE_INT64\",\n",
    "    \"distilbert-base-uncased\": \"TYPE_INT64\"  # DistilBERT (smaller)\n",
    "}\n",
    "def make_models_triton_ready(registered_model_name,registered_model_version,model_type,\n",
    "                                    source_folder,target_folder):   \n",
    "    if os.path.exists(target_folder):\n",
    "        shutil.rmtree(target_folder)    \n",
    "    model_dir = target_folder\n",
    "    data_type = DATA_TYPES[model_type]\n",
    "\n",
    "    \"\"\"\n",
    "    Creates a minimal Triton config.pbtxt for an ONNX model with BERT-like inputs.\n",
    "    Adjust if your model uses different inputs or outputs.\n",
    "    \"\"\"\n",
    "    config_distilbert_base_uncased = f\"\"\"\n",
    "name: \"{registered_model_name}\"\n",
    "platform: \"onnxruntime_onnx\"\n",
    "max_batch_size: 0\n",
    "\n",
    "input [\n",
    "  {{\n",
    "    name: \"input_ids\"\n",
    "    data_type: {data_type}\n",
    "    dims: [ -1, -1 ]\n",
    "  }},\n",
    "  {{\n",
    "    name: \"attention_mask\"\n",
    "    data_type: {data_type}\n",
    "    dims: [ -1, -1 ]\n",
    "  }}\n",
    "]\n",
    "\n",
    "output [\n",
    "  {{\n",
    "    name: \"logits\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ -1, 2 ]\n",
    "  }}\n",
    "version_policy {{\n",
    "      specific {{\n",
    "         versions: [{registered_model_version}]\n",
    "      }}\n",
    "    }}\n",
    "]\n",
    "\"\"\"\n",
    "    config_bert_base_uncased = f\"\"\"\n",
    "    name: \"{registered_model_name}\"\n",
    "    platform: \"onnxruntime_onnx\"\n",
    "    max_batch_size: 0\n",
    "\n",
    "    input [\n",
    "      {{\n",
    "        name: \"input_ids\"\n",
    "        data_type: {data_type}\n",
    "        dims: [ -1, -1 ]\n",
    "      }},\n",
    "      {{\n",
    "        name: \"attention_mask\"\n",
    "        data_type: {data_type}\n",
    "        dims: [ -1, -1 ]\n",
    "      }},\n",
    "      {{\n",
    "       name: \"token_type_ids\"\n",
    "       data_type: {data_type}\n",
    "       dims: [ -1, -1 ]\n",
    "      }}\n",
    "\n",
    "    ]\n",
    "\n",
    "    output [\n",
    "      {{\n",
    "        name: \"logits\"\n",
    "        data_type: TYPE_FP32\n",
    "        dims: [ -1, 2 ]\n",
    "      }}\n",
    "    ]\n",
    "    version_policy {{\n",
    "      specific {{\n",
    "         versions: [{registered_model_version}]\n",
    "      }}\n",
    "    }}\n",
    "    \"\"\"\n",
    "    CONFIG_MAP = {\n",
    "        \"bert-base-uncased\": config_bert_base_uncased,\n",
    "        \"distilbert-base-uncased\": config_distilbert_base_uncased\n",
    "    }\n",
    "\n",
    "\n",
    "        \n",
    "    config_text = CONFIG_MAP[model_type]\n",
    "    config_path = os.path.join(source_folder, \"config.pbtxt\")\n",
    "    with open(config_path, \"w\") as f:\n",
    "            f.write(config_text.strip() + \"\\n\")\n",
    "    if not os.path.exists(model_dir):       \n",
    "        for item in os.listdir(source_folder):\n",
    "            src_path = os.path.join(source_folder, item)\n",
    "            dst_path = os.path.join(target_folder, item)\n",
    "    \n",
    "            if os.path.isdir(source_folder):\n",
    "                shutil.copytree(source_folder, model_dir, dirs_exist_ok=True)\n",
    "            else:\n",
    "                shutil.copy2(source_folder, model_dir)        \n",
    "    else:\n",
    "        print(f\"{model_dir} already exists\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adcdb080-7a9d-4d45-a73e-703a073b6306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nregistered_model_name=\"test-model\"\\nregistered_model_version=20\\nmodel_type=\"bert-base-uncased\"\\nsource_folder=BERT_LOCAL_FOLDER\\ntarget_folder=f\"/mnt/imported/data/triton-dev-ds/models/pre-load/{registered_model_name}/{registered_model_version}\"\\n\\nif os.path.exists(target_folder):\\n    shutil.rmtree(target_folder)\\n    \\nmake_models_triton_ready(model_name,model_version,model_type,BERT_LOCAL_FOLDER,target_folder)\\nprint(f\"Target folder is {target_folder}\")\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "registered_model_name=\"test-model\"\n",
    "registered_model_version=20\n",
    "model_type=\"bert-base-uncased\"\n",
    "source_folder=BERT_LOCAL_FOLDER\n",
    "target_folder=f\"/mnt/imported/data/triton-dev-ds/models/pre-load/{registered_model_name}/{registered_model_version}\"\n",
    "\n",
    "if os.path.exists(target_folder):\n",
    "    shutil.rmtree(target_folder)\n",
    "    \n",
    "make_models_triton_ready(model_name,model_version,model_type,BERT_LOCAL_FOLDER,target_folder)\n",
    "print(f\"Target folder is {target_folder}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e08ad51-f920-47a0-a161-dfe19448f805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent Run Id 799d97a741c245febbecae37280c9853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/09 16:39:46 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runs_uri: runs:/799d97a741c245febbecae37280c9853/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/09 16:39:49 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: BERT-BASED, version 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Child Run Id 799d97a741c245febbecae37280c9853\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e84924d134349ef900c0ced700b4051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/09 16:40:27 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runs_uri: runs:/ca37a42e7d8346bab6ffcc894e5915c1/triton_model_artifacts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/09 16:40:28 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: BERT-BASED-CLIENT, version 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: BERT-BASED-CLIENT\n",
      "Version: 1\n",
      "Status: READY\n",
      "🏃 View run merciful-shrew-877 at: http://127.0.0.1:8765/#/experiments/275/runs/ca37a42e7d8346bab6ffcc894e5915c1\n",
      "🧪 View experiment at: http://127.0.0.1:8765/#/experiments/275\n",
      "🏃 View run secretive-hen-167 at: http://127.0.0.1:8765/#/experiments/275/runs/799d97a741c245febbecae37280c9853\n",
      "🧪 View experiment at: http://127.0.0.1:8765/#/experiments/275\n"
     ]
    }
   ],
   "source": [
    "# Start an MLflow run context and log the llama2-7B model wrapper along with the param-included signature to\n",
    "# allow for overriding parameters at inference time\n",
    "source_folder=BERT_LOCAL_FOLDER\n",
    "DEV_TRITON_BASE_FOLDER = \"/mnt/imported/data/triton-dev-ds/models/pre-load/\"\n",
    "os.environ['MLFLOW_ENABLE_PROXY_MULTIPART_UPLOAD'] = \"true\"\n",
    "LOG_LLM_ARTIFACTS_TO_MLFLOW=True\n",
    "TRITON_INFERENCE_SERVER=\"triton-domino-pre-load-inference\"\n",
    "model_type=\"bert-base-uncased\"\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "with mlflow.start_run(experiment_id=experiment_id) as parent_run:\n",
    "    parent_run_id = parent_run.info.run_id\n",
    "    print(f\"Parent Run Id {parent_run_id}\")\n",
    "    # Save parent model\n",
    "    model_info = mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"\",\n",
    "        python_model=EmptyTritonModel(),\n",
    "        artifacts={}\n",
    "    )\n",
    "    \n",
    "    runs_uri = model_info.model_uri\n",
    "    print(\"runs_uri:\", runs_uri)\n",
    "    model_src = RunsArtifactRepository.get_underlying_uri(runs_uri)\n",
    "    mv = client.create_model_version(registered_model_name, model_src, parent_run_id,tags={\"is_parent\":\"true\"})\n",
    "\n",
    "    \n",
    "    target_folder=os.path.join(DEV_TRITON_BASE_FOLDER,mv.name,mv.version)\n",
    "    \n",
    "    make_models_triton_ready(mv.name,mv.version,model_type,source_folder,target_folder)\n",
    "    \n",
    "    if LOG_LLM_ARTIFACTS_TO_MLFLOW:\n",
    "        mlflow.log_artifacts(source_folder,artifact_path=\"model\")\n",
    "        \n",
    "    # Start child run\n",
    "    with mlflow.start_run(experiment_id=experiment_id,parent_run_id=parent_run_id, nested=True) as child_run:\n",
    "        child_run_id = child_run.info.run_id\n",
    "        mlflow.log_param(\"parent_run_id\",parent_run_id)\n",
    "        mlflow.log_param(\"triton_model_name\",mv.name)\n",
    "        mlflow.log_param(\"triton_model_version\",mv.version)\n",
    "        print(f\"Child Run Id {parent_run_id}\")\n",
    "        model_context = {\n",
    "            \"parent_run_id\":parent_run_id,\n",
    "            \"inference_server_name\":TRITON_INFERENCE_SERVER,\n",
    "            \"model_name\":mv.name,\n",
    "            \"model_version\":mv.version\n",
    "        }\n",
    "        config_path = \"/tmp/model_context.json\"\n",
    "        with open(config_path, \"w\") as f:\n",
    "            json.dump(model_context, f)\n",
    "            \n",
    "        model_info = mlflow.pyfunc.log_model(\n",
    "            artifact_path=\"triton_model_artifacts\",\n",
    "            python_model=TritonModel(),\n",
    "            artifacts={\"model_context\": config_path}\n",
    "        )\n",
    "    \n",
    "        runs_uri = model_info.model_uri\n",
    "        print(\"runs_uri:\", runs_uri)\n",
    "\n",
    "        model_src = RunsArtifactRepository.get_underlying_uri(runs_uri)\n",
    "        mv = client.create_model_version(client_registered_model_name, model_src, child_run_id,tags={\"is_triton_client\":\"true\"})\n",
    "\n",
    "        print(\"Name:\", mv.name)\n",
    "        print(\"Version:\", mv.version)\n",
    "        print(\"Status:\", mv.status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac7672e-c0f5-4fd1-8888-84818d15942c",
   "metadata": {},
   "source": [
    "### Test TritonModel class locally\n",
    "Download it from model registry\n",
    "load_context called automatically and it sees the same mount that is shared between wks and model api\n",
    "predict call will interpret the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "190d3874-2721-4831-857b-78546bfb9e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models:/BERT-BASED-CLIENT/latest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/mlflow/store/artifact/utils/models.py:31: FutureWarning: ``mlflow.tracking.client.MlflowClient.get_latest_versions`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/latest/model-registry.html#migrating-from-stages\n",
      "  latest = client.get_latest_versions(name, None if stage is None else [stage])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "496918435a844d9d8c73f7cbe3467ea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import mlflow.pyfunc\n",
    "os.environ['MLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR']=\"true\"\n",
    "os.environ['inference-proxy-service']=\"https://inference-proxy-service.domino-inference-dev.svc.cluster.local:8443\"\n",
    "# Set model URI (update with your MLflow model registry path)\n",
    "model_uri = f\"models:/{client_registered_model_name}/latest\"  # Example for a registry model\n",
    "print(model_uri)\n",
    "# model_uri = \"runs:/your_run_id/model\"  # If stored in a specific run\n",
    "# Load the MLflow model\n",
    "model = mlflow.pyfunc.load_model(model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ce7ae4d-d490-497d-88c1-d3a8d3fca957",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "payload={  \n",
    "    \"payload\": {\n",
    "       \"inputs\": [\n",
    "            {\n",
    "                \"name\": \"input_ids\",\n",
    "                \"shape\": [1, 8],\n",
    "                \"datatype\": \"INT64\",\n",
    "                \"data\": [101, 1045, 2293, 2023, 3185, 999, 102, 0]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"attention_mask\",\n",
    "                \"shape\": [1, 8],\n",
    "                \"datatype\": \"INT64\",\n",
    "                \"data\": [1, 1, 1, 1, 1, 1, 1, 0]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"token_type_ids\",\n",
    "                \"shape\": [1, 8],\n",
    "                \"datatype\": \"INT64\",\n",
    "                \"data\": [0, 0, 0, 0, 0, 0, 0, 0]\n",
    "            }\n",
    "      ]\n",
    "    }\n",
    "    \n",
    "  }\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "813aefc7-4e69-4422-92e7-e31855ba70a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Called predict\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py:1063: InsecureRequestWarning: Unverified HTTPS request is being made to host 'inference-proxy-service.domino-inference-dev.svc.cluster.local'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status_code': 200,\n",
       " 'body': {'status_code': 200,\n",
       "  'result': {'model_name': 'BERT-BASED',\n",
       "   'model_version': '1',\n",
       "   'outputs': [{'name': 'logits',\n",
       "     'datatype': 'FP32',\n",
       "     'shape': [1, 2],\n",
       "     'data': [-0.13359245657920837, -0.026861200109124184]}]}}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a961916-4fee-4631-ac32-7e069576536a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py:1063: InsecureRequestWarning: Unverified HTTPS request is being made to host 'inference-proxy-service.domino-inference-dev.svc.cluster.local'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"{'status': 'Healthy'}\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://inference-proxy-service.domino-inference-dev.svc.cluster.local:8443/healthz\"\n",
    "resp = requests.get(url,verify=False)\n",
    "resp.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5389942e-2ae5-4df6-bbf1-806b92fa966d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
