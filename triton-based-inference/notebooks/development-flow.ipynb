{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd68380-47c2-45cf-b4cd-3cf1fe6412f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install necessary libraries\n",
    "!pip install --upgrade torch transformers accelerate optimum[onxx] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7cfa69-93d8-4969-8191-ef97a82d90a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install onnx onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8d6159f-2be3-40f9-ae87-2d8acc19da6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import mlflow\n",
    "import requests\n",
    "from mlflow.exceptions import MlflowException\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.store.artifact.runs_artifact_repo import RunsArtifactRepository\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from optimum.exporters.tasks import TasksManager\n",
    "from optimum.exporters.onnx import export\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a335f573-570e-4339-b443-1937b08aeda1",
   "metadata": {},
   "source": [
    "### Download models from hugging face\n",
    "In this step we download two models from Hugging Face\n",
    "1. Bert-based\n",
    "2. Distilbert-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f7ff466-b65e-4c3c-af6a-aed659553344",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cpu\"\n",
    "# ONNX opset version (14+ needed for scaled_dot_product_attention)\n",
    "OPSET = 14\n",
    "\n",
    "def download_and_export_onxx(model_name: str, output_dir: Path, task: str = \"text-classification\"):\n",
    "    if not os.path.exists(output_dir):       \n",
    "        os.makedirs(output_dir)\n",
    "    \"\"\"\n",
    "    Export a Hugging Face model to ONNX using Optimum.\n",
    "    \"\"\"\n",
    "    print(f\"Exporting {model_name} to ONNX for task={task} -> {output_dir}\")\n",
    "\n",
    "    # 1. Load model & tokenizer\n",
    "    print(\"Loading model...\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=2,  # Adjust to your actual classification labels\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # 2. Determine model_type (e.g., \"bert\", \"distilbert\")\n",
    "    model_type = model.config.model_type\n",
    "    \n",
    "\n",
    "    # 3. Build the exporter config\n",
    "    exporter_config_constructor = TasksManager.get_exporter_config_constructor(\n",
    "        model_type=model_type,\n",
    "        exporter=\"onnx\",\n",
    "        library_name=\"transformers\"  # Explicitly specify \"transformers\"\n",
    "    )\n",
    "    exporter_config = exporter_config_constructor(\n",
    "        model.config,\n",
    "        task=task\n",
    "    )\n",
    "\n",
    "    print(\"Running export...\")\n",
    "    file_path = os.path.join(output_dir,\"model.onnx\")\n",
    "    # 4. Run export\n",
    "    onnx_paths = export(\n",
    "        model=model,\n",
    "        config=exporter_config,\n",
    "        output=Path(file_path),\n",
    "        device=DEVICE,\n",
    "        opset=OPSET\n",
    "    )\n",
    "    print(\"Saving tokenizer...\")\n",
    "    tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64f74f2f-273a-4bdc-967d-1b53cdaec80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT = \"bert-base-uncased\"\n",
    "DISTILBERT = \"distilbert-base-uncased\"\n",
    "TASK = \"text-classification\"\n",
    "BERT_LOCAL_FOLDER = f\"/home/ubuntu/mymodels/{BERT}\"\n",
    "DISTILBERT_LOCAL_FOLDER = f\"/home/ubuntu/mymodels/{DISTILBERT}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "618b449d-6ea6-4bce-8b97-999f62cf2dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting bert-base-uncased to ONNX for task=text-classification -> /home/ubuntu/mymodels/bert-base-uncased\n",
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running export...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving tokenizer...\n",
      "Exporting bert-base-uncased to ONNX for task=text-classification -> /home/ubuntu/mymodels/distilbert-base-uncased\n",
      "Loading model...\n",
      "Running export...\n",
      "Saving tokenizer...\n"
     ]
    }
   ],
   "source": [
    "download_and_export_onxx(BERT, BERT_LOCAL_FOLDER,TASK)   \n",
    "download_and_export_onxx(BERT, DISTILBERT_LOCAL_FOLDER,TASK)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9a6cc3-1426-4034-bd0d-1cb5e7aa5f26",
   "metadata": {},
   "source": [
    "## Prepare to register to MLflow\n",
    "\n",
    "`EmptyTritonModel` only exists so we can register the binaries of large model. This model will not be deployed. The dataset hosting the model are stored in the folder location \n",
    "`<DATASET>/models/<registered_model_name>/<registered_model_version>`\n",
    "\n",
    "The values for the `registered_model_name` and `registered_model_version` are the values for the `EmptyTritonModel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b89eead-8b26-4f12-8af4-d417d9b29078",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmptyTritonModel(mlflow.pyfunc.PythonModel):\n",
    "    def load_context(self, context):\n",
    "        return\n",
    "    def predict(self, context, model_input, params=None):\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89793405-1c7f-4b7d-8708-c5dd49e32529",
   "metadata": {},
   "source": [
    "The `TritonModel` is the actual model which gets deployed. No model binaries are registered with it. Instead it is merely passed the following parameters in the configuration\n",
    "1. `inference_server_name` - This is the Triton Inference Server where the models are deployed.\n",
    "2. `model_name` - The `registered model name` for the associated `EmptyTritonModel`\n",
    "3. `model_name` - The `registered model version` for the associated `EmptyTritonModel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10a105b1-7a42-4d51-8f8e-10cdd547a29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TritonModel(mlflow.pyfunc.PythonModel):\n",
    "    import json\n",
    "    import requests\n",
    "    def load_context(self, context):\n",
    "        with open(context.artifacts[\"model_context\"], \"r\") as f:\n",
    "            cfg = json.load(f)\n",
    "        #self.proxy_service = cfg[\"proxy_service\"]\n",
    "        self.proxy_service = os.getenv(\"inference-proxy-service\",\"\")\n",
    "        self.inference_server_name = cfg[\"inference_server_name\"]\n",
    "        self.model_name = cfg[\"model_name\"]\n",
    "        self.model_version = cfg[\"model_version\"]\n",
    "\n",
    "    def predict(self, context, model_input, params=None):\n",
    "        print(\"Called predict\")\n",
    "        triton_model_input = {\n",
    "            \"inference_server_name\": self.inference_server_name,\n",
    "            \"model_name\": self.model_name,\n",
    "            \"model_version\": self.model_version,\n",
    "            \"payload\": model_input[\"payload\"]\n",
    "        }\n",
    "        url = f\"{self.proxy_service}/predict\"\n",
    "        if not self.proxy_service:\n",
    "            return {\n",
    "                \"status_code\": 400,\n",
    "                \"body\": \"Proxy Service Variable not set\"\n",
    "            }            \n",
    "        else:\n",
    "            resp = requests.post(url, json=triton_model_input)\n",
    "            return {\n",
    "                \"status_code\": resp.status_code,\n",
    "                \"body\": resp.json()\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6c23db-e982-4253-bc5b-0db31c8509f0",
   "metadata": {},
   "source": [
    "MLflow code to create experiement and register model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d58a32e-4148-446e-98b7-846466b13430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_experiment(name: str) -> str:\n",
    "    # Try to get the experiment\n",
    "    experiment = mlflow.get_experiment_by_name(name)\n",
    "    if experiment:\n",
    "        return experiment.experiment_id\n",
    "\n",
    "    # Otherwise, create it\n",
    "    experiment_id = mlflow.create_experiment(name)\n",
    "    return experiment_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0b7a370-24cb-4d56-9f76-000dfd2a3827",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_registered_model(name: str):\n",
    "    client = MlflowClient()\n",
    "    try:\n",
    "        # Try to get the model details\n",
    "        model = client.get_registered_model(name)\n",
    "        print(f\"Model '{name}' already registered.\")\n",
    "    except MlflowException as e:\n",
    "        if \"RESOURCE_DOES_NOT_EXIST\" in str(e):\n",
    "            # Model doesn't exist â€” create it\n",
    "            model = client.create_registered_model(name)\n",
    "            print(f\"Model '{name}' created.\")\n",
    "        else:\n",
    "            raise e\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b65517-e306-4cc8-8046-524736ee33b8",
   "metadata": {},
   "source": [
    "Specify three variables:\n",
    "1. Experiment Name where the MLflow runs are registered\n",
    "2. Registered Model Name - for the `EmptyTritonModel` which stores the model binaries\n",
    "3. Client Registered Model Name - for the `TritonModel` which serves as a client model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25a2c5ca-c338-4d33-bd44-cf00e9f3a996",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name=\"TRITON-INFERENCE-SERVER-MODELS\"\n",
    "registered_model_name=\"BERT-BASED\"\n",
    "client_registered_model_name=\"BERT-BASED-CLIENT\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30c2b15f-0ce8-4ce3-9179-0eba3fa41edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 'BERT-BASED' already registered.\n",
      "Model 'BERT-BASED-CLIENT' already registered.\n"
     ]
    }
   ],
   "source": [
    "## Create Experiment and Models\n",
    "experiment_id = get_or_create_experiment(experiment_name)\n",
    "registered_model = get_or_create_registered_model(registered_model_name)\n",
    "client_registered_model = get_or_create_registered_model(client_registered_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0a367f-01c9-4616-a975-95f58bde7861",
   "metadata": {},
   "source": [
    "Convert the downloaded models to onxx format and store them to the target folder location - <DEV_DATASET>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee30f87b-c71e-478a-9419-cdcc6f8156cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TYPES = {\n",
    "    \"bert-base-uncased\": \"TYPE_INT64\",\n",
    "    \"distilbert-base-uncased\": \"TYPE_INT64\"  # DistilBERT (smaller)\n",
    "}\n",
    "def make_models_triton_ready(registered_model_name,registered_model_version,model_type,\n",
    "                                    source_folder,target_folder):   \n",
    "    if os.path.exists(target_folder):\n",
    "        shutil.rmtree(target_folder)    \n",
    "    model_dir = target_folder\n",
    "    data_type = DATA_TYPES[model_type]\n",
    "\n",
    "    \"\"\"\n",
    "    Creates a minimal Triton config.pbtxt for an ONNX model with BERT-like inputs.\n",
    "    Adjust if your model uses different inputs or outputs.\n",
    "    \"\"\"\n",
    "    config_distilbert_base_uncased = f\"\"\"\n",
    "name: \"{registered_model_name}\"\n",
    "platform: \"onnxruntime_onnx\"\n",
    "max_batch_size: 0\n",
    "\n",
    "input [\n",
    "  {{\n",
    "    name: \"input_ids\"\n",
    "    data_type: {data_type}\n",
    "    dims: [ -1, -1 ]\n",
    "  }},\n",
    "  {{\n",
    "    name: \"attention_mask\"\n",
    "    data_type: {data_type}\n",
    "    dims: [ -1, -1 ]\n",
    "  }}\n",
    "]\n",
    "\n",
    "output [\n",
    "  {{\n",
    "    name: \"logits\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [ -1, 2 ]\n",
    "  }}\n",
    "version_policy {{\n",
    "      specific {{\n",
    "         versions: [{registered_model_version}]\n",
    "      }}\n",
    "    }}\n",
    "]\n",
    "\"\"\"\n",
    "    config_bert_base_uncased = f\"\"\"\n",
    "    name: \"{registered_model_name}\"\n",
    "    platform: \"onnxruntime_onnx\"\n",
    "    max_batch_size: 0\n",
    "\n",
    "    input [\n",
    "      {{\n",
    "        name: \"input_ids\"\n",
    "        data_type: {data_type}\n",
    "        dims: [ -1, -1 ]\n",
    "      }},\n",
    "      {{\n",
    "        name: \"attention_mask\"\n",
    "        data_type: {data_type}\n",
    "        dims: [ -1, -1 ]\n",
    "      }},\n",
    "      {{\n",
    "       name: \"token_type_ids\"\n",
    "       data_type: {data_type}\n",
    "       dims: [ -1, -1 ]\n",
    "      }}\n",
    "\n",
    "    ]\n",
    "\n",
    "    output [\n",
    "      {{\n",
    "        name: \"logits\"\n",
    "        data_type: TYPE_FP32\n",
    "        dims: [ -1, 2 ]\n",
    "      }}\n",
    "    ]\n",
    "    version_policy {{\n",
    "      specific {{\n",
    "         versions: [{registered_model_version}]\n",
    "      }}\n",
    "    }}\n",
    "    \"\"\"\n",
    "    CONFIG_MAP = {\n",
    "        \"bert-base-uncased\": config_bert_base_uncased,\n",
    "        \"distilbert-base-uncased\": config_distilbert_base_uncased\n",
    "    }\n",
    "\n",
    "\n",
    "        \n",
    "    config_text = CONFIG_MAP[model_type]\n",
    "    config_path = os.path.join(source_folder, \"config.pbtxt\")\n",
    "    with open(config_path, \"w\") as f:\n",
    "            f.write(config_text.strip() + \"\\n\")\n",
    "    if not os.path.exists(model_dir):       \n",
    "        for item in os.listdir(source_folder):\n",
    "            src_path = os.path.join(source_folder, item)\n",
    "            dst_path = os.path.join(target_folder, item)\n",
    "    \n",
    "            if os.path.isdir(source_folder):\n",
    "                shutil.copytree(source_folder, model_dir, dirs_exist_ok=True)\n",
    "            else:\n",
    "                shutil.copy2(source_folder, model_dir)        \n",
    "    else:\n",
    "        print(f\"{model_dir} already exists\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adcdb080-7a9d-4d45-a73e-703a073b6306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nregistered_model_name=\"test-model\"\\nregistered_model_version=20\\nmodel_type=\"bert-base-uncased\"\\nsource_folder=BERT_LOCAL_FOLDER\\ntarget_folder=f\"/mnt/imported/data/triton-dev-ds/models/pre-load/{registered_model_name}/{registered_model_version}\"\\n\\nif os.path.exists(target_folder):\\n    shutil.rmtree(target_folder)\\n    \\nmake_models_triton_ready(model_name,model_version,model_type,BERT_LOCAL_FOLDER,target_folder)\\nprint(f\"Target folder is {target_folder}\")\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "registered_model_name=\"test-model\"\n",
    "registered_model_version=20\n",
    "model_type=\"bert-base-uncased\"\n",
    "source_folder=BERT_LOCAL_FOLDER\n",
    "target_folder=f\"/mnt/imported/data/triton-dev-ds/models/pre-load/{registered_model_name}/{registered_model_version}\"\n",
    "\n",
    "if os.path.exists(target_folder):\n",
    "    shutil.rmtree(target_folder)\n",
    "    \n",
    "make_models_triton_ready(model_name,model_version,model_type,BERT_LOCAL_FOLDER,target_folder)\n",
    "print(f\"Target folder is {target_folder}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e08ad51-f920-47a0-a161-dfe19448f805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent Run Id 0037760ffbe74a258c0b7ee8982f2ece\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/08 20:24:01 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/05/08 20:24:02 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: BERT-BASED, version 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runs_uri: runs:/0037760ffbe74a258c0b7ee8982f2ece/\n",
      "Child Run Id 0037760ffbe74a258c0b7ee8982f2ece\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a73e9d4065e4f0c8c51d6860ceef2f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/08 20:24:41 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2025/05/08 20:24:42 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: BERT-BASED-CLIENT, version 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runs_uri: runs:/8b67cfc67e4f4417810412ab16f9a867/triton_model_artifacts\n",
      "Name: BERT-BASED-CLIENT\n",
      "Version: 7\n",
      "Status: READY\n",
      "ðŸƒ View run suave-rat-564 at: http://127.0.0.1:8768/#/experiments/1/runs/8b67cfc67e4f4417810412ab16f9a867\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:8768/#/experiments/1\n",
      "ðŸƒ View run luxuriant-wolf-763 at: http://127.0.0.1:8768/#/experiments/1/runs/0037760ffbe74a258c0b7ee8982f2ece\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:8768/#/experiments/1\n"
     ]
    }
   ],
   "source": [
    "# Start an MLflow run context and log the llama2-7B model wrapper along with the param-included signature to\n",
    "# allow for overriding parameters at inference time\n",
    "source_folder=BERT_LOCAL_FOLDER\n",
    "DEV_TRITON_BASE_FOLDER = \"/mnt/imported/data/triton-dev-ds/models/pre-load/\"\n",
    "os.environ['MLFLOW_ENABLE_PROXY_MULTIPART_UPLOAD'] = \"true\"\n",
    "LOG_LLM_ARTIFACTS_TO_MLFLOW=True\n",
    "TRITON_INFERENCE_SERVER=\"triton-domino-pre-load-inference\"\n",
    "model_type=\"bert-base-uncased\"\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "with mlflow.start_run(experiment_id=experiment_id) as parent_run:\n",
    "    parent_run_id = parent_run.info.run_id\n",
    "    print(f\"Parent Run Id {parent_run_id}\")\n",
    "    # Save parent model\n",
    "    model_info = mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"\",\n",
    "        python_model=EmptyTritonModel(),\n",
    "        artifacts={}\n",
    "    )\n",
    "    \n",
    "    runs_uri = model_info.model_uri\n",
    "    print(\"runs_uri:\", runs_uri)\n",
    "    model_src = RunsArtifactRepository.get_underlying_uri(runs_uri)\n",
    "    mv = client.create_model_version(registered_model_name, model_src, parent_run_id,tags={\"is_parent\":\"true\"})\n",
    "\n",
    "    \n",
    "    target_folder=os.path.join(DEV_TRITON_BASE_FOLDER,mv.name,mv.version)\n",
    "    \n",
    "    make_models_triton_ready(mv.name,mv.version,model_type,source_folder,target_folder)\n",
    "    \n",
    "    if LOG_LLM_ARTIFACTS_TO_MLFLOW:\n",
    "        mlflow.log_artifacts(source_folder,artifact_path=\"model\")\n",
    "        \n",
    "    # Start child run\n",
    "    with mlflow.start_run(experiment_id=experiment_id,parent_run_id=parent_run_id, nested=True) as child_run:\n",
    "        child_run_id = child_run.info.run_id\n",
    "        mlflow.log_param(\"parent_run_id\",parent_run_id)\n",
    "        mlflow.log_param(\"triton_model_name\",mv.name)\n",
    "        mlflow.log_param(\"triton_model_version\",mv.version)\n",
    "        print(f\"Child Run Id {parent_run_id}\")\n",
    "        model_context = {\n",
    "            \"parent_run_id\":parent_run_id,\n",
    "            \"inference_server_name\":TRITON_INFERENCE_SERVER,\n",
    "            \"model_name\":mv.name,\n",
    "            \"model_version\":mv.version\n",
    "        }\n",
    "        config_path = \"/tmp/model_context.json\"\n",
    "        with open(config_path, \"w\") as f:\n",
    "            json.dump(model_context, f)\n",
    "            \n",
    "        model_info = mlflow.pyfunc.log_model(\n",
    "            artifact_path=\"triton_model_artifacts\",\n",
    "            python_model=TritonModel(),\n",
    "            artifacts={\"model_context\": config_path}\n",
    "        )\n",
    "    \n",
    "        runs_uri = model_info.model_uri\n",
    "        print(\"runs_uri:\", runs_uri)\n",
    "\n",
    "        model_src = RunsArtifactRepository.get_underlying_uri(runs_uri)\n",
    "        mv = client.create_model_version(client_registered_model_name, model_src, child_run_id,tags={\"is_triton_client\":\"true\"})\n",
    "\n",
    "        print(\"Name:\", mv.name)\n",
    "        print(\"Version:\", mv.version)\n",
    "        print(\"Status:\", mv.status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac7672e-c0f5-4fd1-8888-84818d15942c",
   "metadata": {},
   "source": [
    "### Test TritonModel class locally\n",
    "Download it from model registry\n",
    "load_context called automatically and it sees the same mount that is shared between wks and model api\n",
    "predict call will interpret the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "190d3874-2721-4831-857b-78546bfb9e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models:/BERT-BASED-CLIENT/latest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/mlflow/store/artifact/utils/models.py:31: FutureWarning: ``mlflow.tracking.client.MlflowClient.get_latest_versions`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/latest/model-registry.html#migrating-from-stages\n",
      "  latest = client.get_latest_versions(name, None if stage is None else [stage])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85c024c9cb694d69b1cf1ddc68c8768a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import mlflow.pyfunc\n",
    "os.environ['MLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR']=\"true\"\n",
    "os.environ['inference-proxy-service']=\"http://inference-proxy-service.domino-inference-dev.svc.cluster.local:8000\"\n",
    "# Set model URI (update with your MLflow model registry path)\n",
    "model_uri = f\"models:/{client_registered_model_name}/latest\"  # Example for a registry model\n",
    "print(model_uri)\n",
    "# model_uri = \"runs:/your_run_id/model\"  # If stored in a specific run\n",
    "# Load the MLflow model\n",
    "model = mlflow.pyfunc.load_model(model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ce7ae4d-d490-497d-88c1-d3a8d3fca957",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "payload={  \n",
    "    \"payload\": {\n",
    "       \"inputs\": [\n",
    "            {\n",
    "                \"name\": \"input_ids\",\n",
    "                \"shape\": [1, 8],\n",
    "                \"datatype\": \"INT64\",\n",
    "                \"data\": [101, 1045, 2293, 2023, 3185, 999, 102, 0]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"attention_mask\",\n",
    "                \"shape\": [1, 8],\n",
    "                \"datatype\": \"INT64\",\n",
    "                \"data\": [1, 1, 1, 1, 1, 1, 1, 0]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"token_type_ids\",\n",
    "                \"shape\": [1, 8],\n",
    "                \"datatype\": \"INT64\",\n",
    "                \"data\": [0, 0, 0, 0, 0, 0, 0, 0]\n",
    "            }\n",
    "      ]\n",
    "    }\n",
    "    \n",
    "  }\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "813aefc7-4e69-4422-92e7-e31855ba70a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Called predict\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status_code': 200,\n",
       " 'body': {'status_code': 200,\n",
       "  'result': {'model_name': 'BERT-BASED',\n",
       "   'model_version': '10',\n",
       "   'outputs': [{'name': 'logits',\n",
       "     'datatype': 'FP32',\n",
       "     'shape': [1, 2],\n",
       "     'data': [0.07767994701862335, 0.16845941543579102]}]}}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a961916-4fee-4631-ac32-7e069576536a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
