{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70fffce8-04ed-4818-8c88-f583633ff927",
   "metadata": {},
   "source": [
    "### Register the Model \n",
    "\n",
    "1. Create the model schema\n",
    "2. Register the model code `LLMModel` class to the Model Registry\n",
    "   - Note the `load_context` function and how the model api loads from the dataset mount (Mutation adds this)\n",
    "   - And notice the `predict` function on how it interprets the input payload\n",
    "4. Note the model name and model version and head over to deploy it using the model api endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "070411a9-b571-423a-b1f2-e0cb0e70bb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import os\n",
    "import accelerate\n",
    "import json\n",
    "import mlflow\n",
    "import transformers\n",
    "import os\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "81d7c604-c2ff-439b-9baa-44e4a0618b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the default dataset of the project to download the model. But it can be any dataset you have access to\n",
    "def get_download_dataset_folder(model_name):\n",
    "    ds_dir = os.environ['DOMINO_DATASETS_DIR']\n",
    "    ds_name = os.environ['DOMINO_PROJECT_NAME']\n",
    "    download_ds_dir = f\"{ds_dir}/{ds_name}/{model_name}\"\n",
    "    return download_ds_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "779f4dc9-c471-4ea5-927e-97a7d7436bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/gemma-2b\"\n",
    "model_path = get_download_dataset_folder(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "21595c0a-c747-4429-9c91-21f41d2702b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"gemma2b\"\n",
    "registered_model_name = experiment_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a81aa3f5-4473-415e-8e2f-eb127bc7184a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LLMModel(mlflow.pyfunc.PythonModel):\n",
    "        from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "        import pandas\n",
    "        import torch\n",
    "        import os\n",
    "        import accelerate\n",
    "        def load_context(self, context):\n",
    "            metadata={}\n",
    "            print(\"Context Artifacts (Only Metadata)\")\n",
    "            print(context.artifacts)\n",
    "\n",
    "            \n",
    "            with open(context.artifacts[\"model_binaries_path\"], \"r\") as f:\n",
    "                metadata = json.load(f)\n",
    "               \n",
    "            print(\"Loaded Metadata\")\n",
    "            print(metadata)\n",
    "            model_path = metadata[\"model_path\"]\n",
    "            \n",
    "            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "            #model_path = save_path  # Change this to your local model directory\n",
    "\n",
    "            print(f\"Now load the model from the path obtained from the metadata {model_path}\")\n",
    "            model = AutoModelForCausalLM.from_pretrained(model_path, \n",
    "                                                         torch_dtype=torch.float16, \n",
    "                                                         device_map=device)\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "            # Create a text-generation pipeline\n",
    "            self.text_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "            \n",
    "            \n",
    "            #prompt = \"Once upon a time in a distant land,\"\n",
    "            \n",
    "    \n",
    "    \n",
    "        def predict(self, context, model_input, params=None):\n",
    "            \"\"\"\n",
    "            This method generates prediction for the given input.\n",
    "            \"\"\"\n",
    "\n",
    "            prompt = model_input[\"prompt\"]            \n",
    "            input_string = prompt.iloc[0]\n",
    "            output = self.text_generator(input_string, max_length=50, do_sample=True)\n",
    "            return {'text_from_llm': output}\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "257c3025-f8a2-403a-b8fe-12deb7510a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import pandas\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.types import DataType, Schema, ColSpec, ParamSchema, ParamSpec\n",
    "\n",
    "# Define input and output schema\n",
    "input_schema = Schema(\n",
    "    [\n",
    "        ColSpec(DataType.string, \"prompt\"),\n",
    "    ]\n",
    ")\n",
    "output_schema = Schema([ColSpec(DataType.string, \"text_from_llm\")])\n",
    "\n",
    "parameters = ParamSchema(\n",
    "    [       \n",
    "    ]\n",
    ")\n",
    "\n",
    "signature = ModelSignature(inputs=input_schema, outputs=output_schema, params=parameters)\n",
    "\n",
    "\n",
    "# Define input example\n",
    "input_example = pd.DataFrame({\"prompt\": [\"Once upon a time in a distant land,\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b3268191-4a3d-45d8-9cf0-3a9ec9b57fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Id is 7\n"
     ]
    }
   ],
   "source": [
    "client = mlflow.MlflowClient()\n",
    "\n",
    "experiment = client.get_experiment_by_name(experiment_name)\n",
    "if not experiment:\n",
    "    experiment_id = client.create_experiment(experiment_name)\n",
    "else:\n",
    "    experiment_id = experiment.experiment_id\n",
    "\n",
    "print(f\"Experiment Id is {experiment_id}\")\n",
    "\n",
    "\n",
    "registered_model = None\n",
    "try:\n",
    "    registered_model = client.create_registered_model(registered_model_name)\n",
    "except:\n",
    "     registered_model = client.get_registered_model(registered_model_name)\n",
    "registered_model = client.get_registered_model(registered_model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f9138a8c-3c65-4632-b4b7-a78fb16110d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import shutil\n",
    "def write_dict_to_model_json(data: dict, path: str):\n",
    "    if os.path.exists(path):\n",
    "        if os.path.isfile(path):\n",
    "            os.remove(path)\n",
    "        else:\n",
    "            shutil.rmtree(path)\n",
    "\n",
    "    # Create the directory\n",
    "    os.makedirs(path, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "    file_path = os.path.join(path, \"model.json\")\n",
    "    with open(file_path, \"w\") as f:\n",
    "        json.dump(data, f, indent=2)  # Pretty-print with 2-space indent\n",
    "    return file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "42221d95-93fd-4eb6-a1ac-5fb60df3fea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08067f4340594b50a15c1d670fa2313d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Artifacts (Only Metadata)\n",
      "{'model_binaries_path': '/tmp/tmplvdg85xs/model/artifacts/model.json'}\n",
      "Loaded Metadata\n",
      "{'model_path': '/mnt/data/deploy_llm/google/gemma-2b'}\n",
      "Now load the model from the path obtained from the metadata /mnt/data/deploy_llm/google/gemma-2b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37051ba607e34a6698cd117ba0572f51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Registered model 'gemma2b' already exists. Creating a new version of this model...\n",
      "2025/03/31 20:36:35 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: gemma2b, version 8\n",
      "Created version '8' of model 'gemma2b'.\n",
      "Registered model 'gemma2b' already exists. Creating a new version of this model...\n",
      "2025/03/31 20:36:36 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: gemma2b, version 9\n",
      "Created version '9' of model 'gemma2b'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: gemma2b\n",
      "Version: 9\n",
      "Description: \n",
      "Status: READY\n",
      "Stage: None\n",
      "🏃 View run illustrious-lark-851 at: http://127.0.0.1:8765/#/experiments/7/runs/4392bf746089483fa3264e5c7f9c3ecb\n",
      "🧪 View experiment at: http://127.0.0.1:8765/#/experiments/7\n"
     ]
    }
   ],
   "source": [
    "## Create a json which contains the path of the model artifacts\n",
    "\n",
    "model_artifacts={\n",
    "    \"model_path\":model_path\n",
    "}\n",
    "file_path = write_dict_to_model_json(model_artifacts, \"/tmp/mymodel\")\n",
    "#Write to a local location\n",
    "\n",
    "\n",
    "\n",
    "# Get the current base version of torch that is installed, without specific version modifiers\n",
    "torch_version = torch.__version__.split(\"+\")[0]\n",
    "# Start an MLflow run context and log the llama2-7B model wrapper along with the param-included signature to\n",
    "# allow for overriding parameters at inference time\n",
    "with mlflow.start_run(experiment_id=experiment_id) as run:\n",
    "    run_id = run.info.run_id\n",
    "    model_info = mlflow.pyfunc.log_model(        \n",
    "        artifact_path=\"model\",\n",
    "        python_model=LLMModel(),\n",
    "        artifacts={\"model_binaries_path\": file_path},\n",
    "        pip_requirements=[\n",
    "            f\"torch=={torch_version}\",\n",
    "            f\"transformers=={transformers.__version__}\",\n",
    "            f\"accelerate=={accelerate.__version__}\"\n",
    "        ],\n",
    "        input_example=input_example,\n",
    "        signature=signature,\n",
    "        registered_model_name=registered_model_name \n",
    "    )\n",
    "    model_src = f\"runs:/{run_id}/model\"\n",
    "    mv = mlflow.register_model(model_uri=model_src, name=registered_model_name)\n",
    "    print(\"Name: {}\".format(mv.name))\n",
    "    print(\"Version: {}\".format(mv.version))\n",
    "    print(\"Description: {}\".format(mv.description))\n",
    "    print(\"Status: {}\".format(mv.status))\n",
    "    print(\"Stage: {}\".format(mv.current_stage))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882fc0c3-55d6-4055-b7c5-24fa2e2da270",
   "metadata": {},
   "source": [
    "### Test you LLMModel class locally\n",
    "1. Download it from model registry\n",
    "2. `load_context` called automatically and it sees the same mount that is shared between wks and model api\n",
    "3. `predict` call will interpret the input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "30870dfb-cdda-4d55-8a0f-f8baa1424f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models:/gemma2b/9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44eb69f9ef69418a9efcddd9c49d2344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Artifacts (Only Metadata)\n",
      "{'model_binaries_path': '/tmp/tmpeqmph3kq/artifacts/model.json'}\n",
      "Loaded Metadata\n",
      "{'model_path': '/mnt/data/deploy_llm/google/gemma-2b'}\n",
      "Now load the model from the path obtained from the metadata /mnt/data/deploy_llm/google/gemma-2b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35650709490449ce9cb33cad4f12808b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "import mlflow.pyfunc\n",
    "os.environ['MLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR']=\"true\"\n",
    "model_version = \"latest\"\n",
    "#or\n",
    "#model_version = \"5\"\n",
    "model_uri = f\"models:/{registered_model_name}/{mv.version}\"  # Example for a registry model\n",
    "print(model_uri)\n",
    "# Load the MLflow model\n",
    "model = mlflow.pyfunc.load_model(model_uri)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ed632cd2-e743-420f-92fa-8f4b44420cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "result = model.predict({\"prompt\" : \"Once upon a time in a distant land,\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "05f8031a-cbb4-42c9-937f-181f9e5a77da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text_from_llm': [{'generated_text': 'Once upon a time in a distant land, there existed a country whose name was, uh, let’s just say the world knows it well enough. It was a country that had a large empire, made many conquests, and had as much'}]}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbe3a2f-a386-4808-a90f-dd5d8f38a64e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7adce9-d35e-47b8-983a-a5f7f24c4b6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a81143d-d659-44c7-a1da-5bdbb850b67c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
