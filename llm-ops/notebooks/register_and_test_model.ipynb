{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7170cb6c-7da2-4efa-89f7-6166130eda4d",
   "metadata": {},
   "source": [
    "## Register downloaded model to Model Registry\n",
    "\n",
    "For this demo we use `TinyLlama` model. We will assume that the `local_download.ipynb` notebook\n",
    "has executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3db76c-0183-4192-bf63-38536c19c5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "import accelerate\n",
    "import numpy\n",
    "import mlflow\n",
    "from mlflow.exceptions import MlflowException\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.store.artifact.runs_artifact_repo import RunsArtifactRepository\n",
    "from utils import recreate_folder\n",
    "\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaf2187-dcff-4d7e-b368-6308fdb9ab66",
   "metadata": {},
   "source": [
    "## Define the model class\n",
    "\n",
    "Use this pattern to create a Model class. This class is an extention of `mlflow.pyfunc.PythonModel`\n",
    "\n",
    "This class has the following characteristics:\n",
    "\n",
    "1. Model context has an attribute `run_id`. This is the `run_id` associated with this model version but not the `run_id` of the model version itself. \n",
    "2. The model binaries and configs are stored in the artifacts of this referenced `run_id`\n",
    "3. The artifacts in the location `/llm-models/{referenced_run_id}/model`\n",
    "4. The `load_context` method initializes the model\n",
    "5. The `predict` function \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef2a3b4-4e9c-4592-8fff-8abe8c758d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import os\n",
    "import accelerate\n",
    "class LLMModel(mlflow.pyfunc.PythonModel):\n",
    "        from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "        import pandas\n",
    "        import torch\n",
    "        import os\n",
    "        import accelerate\n",
    "        def load_context(self, context):\n",
    "            root_path = os.environ.get(\"LOCAL_ROOT_FOLDER\",\"/\")\n",
    "            model_path = \"llm-models\"\n",
    "            with open(context.artifacts[\"model_context\"], \"r\") as f:\n",
    "                cfg = json.load(f)\n",
    "            self.mlflow_run_id = cfg[\"run_id\"]            \n",
    "            self.absolute_model_path = os.path.join(root_path,model_path,self.mlflow_run_id)\n",
    "            print(os.listdir(self.absolute_model_path))\n",
    "            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "            print(self.absolute_model_path)\n",
    "            model = AutoModelForCausalLM.from_pretrained(self.absolute_model_path, \n",
    "                                                         torch_dtype=torch.float16, \n",
    "                                                         device_map=device)\n",
    "            tokenizer = AutoTokenizer.from_pretrained(self.absolute_model_path)\n",
    "            # Create a text-generation pipeline\n",
    "            self.text_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "            \n",
    "            \n",
    "            #prompt = \"Once upon a time in a distant land,\"\n",
    "            \n",
    "    \n",
    "    \n",
    "        def predict(self, context, model_input, params=None):\n",
    "            \"\"\"\n",
    "            This method generates prediction for the given input.\n",
    "            \"\"\"\n",
    "\n",
    "            prompt = model_input[\"prompt\"]\n",
    "            if hasattr(prompt, \"iloc\"):  # pandas Series\n",
    "                input_string = prompt.iloc[0]\n",
    "            else:  # regular list\n",
    "                input_string = prompt[0]\n",
    "            output = self.text_generator(input_string, max_length=50, do_sample=True)\n",
    "            return {'text_from_llm': output}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766b34ab-3df9-437c-b5fc-b6630023b704",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_experiment(name: str) -> str:\n",
    "    # Try to get the experiment\n",
    "    experiment = mlflow.get_experiment_by_name(name)\n",
    "    if experiment:\n",
    "        return experiment.experiment_id\n",
    "\n",
    "    # Otherwise, create it\n",
    "    experiment_id = mlflow.create_experiment(name)\n",
    "    return experiment_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4bd61f-a86b-4053-ba7f-11e6dcf06669",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_registered_model(name: str):\n",
    "    client = MlflowClient()\n",
    "    try:\n",
    "        # Try to get the model details\n",
    "        model = client.get_registered_model(name)\n",
    "        print(f\"Model '{name}' already registered.\")\n",
    "    except MlflowException as e:\n",
    "        if \"RESOURCE_DOES_NOT_EXIST\" in str(e):\n",
    "            # Model doesn't exist â€” create it\n",
    "            model = client.create_registered_model(name)\n",
    "            print(f\"Model '{name}' created.\")\n",
    "        else:\n",
    "            raise e\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b8f0eb-8bd5-4f8b-8052-bfa49ce2a535",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name=\"TinyLlama-MODEL-REGISTRATION\"\n",
    "registered_model_name=\"TinyLlama-MODEL\"\n",
    "experiment_id = get_or_create_experiment(experiment_name)\n",
    "registered_model = get_or_create_registered_model(registered_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fbdd30-61ca-49b6-a8a1-cb0242229e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_ROOT_FOLDER_PREFIX = \"/mnt/imported/data\" #Use this for Git backed projects\n",
    "#DATASET_ROOT_FOLDER_PREFIX = \"/domino/datasets\" #Use this for DFS based projects\n",
    "MODEL_ROOT_FOLDER = f\"{DATASET_ROOT_FOLDER_PREFIX}/domino-models-dev\"\n",
    "MODEL_SUB_FOLDER = \"llm-models\"\n",
    "LOCAL_MODEL_FOLDER = \"/home/ubuntu/TinyLlama\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41bcbda-cbff-4910-b9d0-c4d9727ea91a",
   "metadata": {},
   "source": [
    "### BUILD AND TEST\n",
    "\n",
    "In the first pass test during DEV phase, \n",
    "```\n",
    "ONLY_LOCAL_TESTING = True\n",
    "```\n",
    "This avoids publishing model binaries to MLFLOW. Because the model is \n",
    "served from the dataset, this works but here in the dev phase you are NOT using Domino as the system of record.\n",
    "\n",
    "Once you are happy with your model, rerun the cells below after setting \n",
    "```\n",
    "ONLY_LOCAL_TESTING = False\n",
    "```\n",
    "This publishes your model binary to MLFLOW. This is the point at which you tell the `prod-deployer` role to deploy this model version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4288497b-5be4-4a72-b652-12886951e924",
   "metadata": {},
   "outputs": [],
   "source": [
    "## When true we do not publish artifacts to MLflow\n",
    "ONLY_LOCAL_TESTING = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b88d2fb-d791-40a9-945e-3cb3090eed0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Neded here for save model to work\n",
    "os.environ[\"LOCAL_ROOT_FOLDER\"]=MODEL_ROOT_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4813ff-54de-45f9-8d84-12a03f19c223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start an MLflow run context and log the llama2-7B model wrapper along with the param-included signature to\n",
    "# allow for overriding parameters at inference time\n",
    "\n",
    "os.environ['MLFLOW_ENABLE_PROXY_MULTIPART_UPLOAD'] = \"true\"\n",
    "torch_version = torch.__version__.split(\"+\")[0]\n",
    "client = MlflowClient()\n",
    "#First register the model binaries\n",
    "with mlflow.start_run(experiment_id=experiment_id) as parent_run:\n",
    "    parent_run_id = parent_run.info.run_id\n",
    "    print(f\"Parent Run Id {parent_run_id}\")\n",
    "    target_local_dir = os.path.join(MODEL_ROOT_FOLDER,MODEL_SUB_FOLDER,parent_run_id)\n",
    "    recreate_folder(target_local_dir)\n",
    "    shutil.copytree(LOCAL_MODEL_FOLDER, target_local_dir, dirs_exist_ok=True)\n",
    "\n",
    "    #KEY DESIGN. THESE BINARIES CAN BE LARGE, YOU DO NOT PUBLISH THEM UNTIL YOU ARE CONFIDENT THE MODEL WORKS\n",
    "    if not ONLY_LOCAL_TESTING:\n",
    "        mlflow.log_artifacts(target_local_dir,artifact_path=\"model\")\n",
    "    \n",
    "    # Start child run\n",
    "    with mlflow.start_run(experiment_id=experiment_id,parent_run_id=parent_run_id, nested=True) as child_run:\n",
    "        child_run_id = child_run.info.run_id\n",
    "        print(f\"Child Run Id {parent_run_id}\")\n",
    "        # Save model config\n",
    "       \n",
    "        model_context = {\n",
    "            \"run_id\":parent_run_id            \n",
    "        }\n",
    "        config_path = \"/tmp/model_context.json\"\n",
    "        with open(config_path, \"w\") as f:\n",
    "            json.dump(model_context, f)\n",
    "        \n",
    "        model_info = mlflow.pyfunc.log_model(\n",
    "            artifact_path=\"\",\n",
    "            python_model=LLMModel(),\n",
    "            artifacts={\"model_context\": config_path},\n",
    "            pip_requirements=[\n",
    "            f\"torch=={torch_version}\",\n",
    "            f\"transformers=={transformers.__version__}\",\n",
    "            f\"accelerate=={accelerate.__version__}\"\n",
    "           ]\n",
    "        )\n",
    "\n",
    "    runs_uri = model_info.model_uri\n",
    "    print(\"runs_uri:\", runs_uri)\n",
    "\n",
    "    model_src = RunsArtifactRepository.get_underlying_uri(runs_uri)\n",
    "    mv = client.create_model_version(registered_model_name, model_src, child_run_id,tags={\"triton_env\":\"domino-triton-dev\"})\n",
    "\n",
    "    print(\"Name:\", mv.name)\n",
    "    print(\"Version:\", mv.version)\n",
    "    print(\"Status:\", mv.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3537f892-9768-4b7b-984a-3dbe9a586985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.pyfunc\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.artifacts import download_artifacts\n",
    "from mlflow.pyfunc import load_model\n",
    "\n",
    "os.environ['MLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR']=\"true\"\n",
    "\n",
    "'''\n",
    "model_uri = f\"models:/{registered_model_name}/latest\"  # Example for a registry model\n",
    "print(model_uri)\n",
    "model = mlflow.pyfunc.load_model(model_uri)\n",
    "'''\n",
    "\n",
    "MODEL_NAME=mv.name\n",
    "MODEL_VERSION=mv.version\n",
    "\n",
    "print(\"v1\")\n",
    "client = MlflowClient()\n",
    "mv = client.get_model_version(name=MODEL_NAME, version=MODEL_VERSION)\n",
    "run_id = mv.run_id\n",
    "print(run_id)\n",
    "\n",
    "\n",
    "\n",
    "#This does not work either\n",
    "#local_path = client.download_artifacts(run_id, \"llm-model\")\n",
    "\n",
    "\n",
    "artifact_uri = client.get_run(run_id).info.artifact_uri\n",
    "local_path = mlflow.artifacts.download_artifacts(f\"{artifact_uri}/llm-model\")\n",
    "model = load_model(local_path)\n",
    "\n",
    "result = model.predict({\"prompt\" : [\"Once upon a time in a distant land,\"]})\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dc932c-a47b-4751-b320-3fd3b47d4fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.pyfunc\n",
    "os.environ['MLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR']=\"true\"\n",
    "os.environ[\"LOCAL_ROOT_FOLDER\"]=MODEL_ROOT_FOLDER\n",
    "MODEL_NAME=mv.name\n",
    "MODEL_VERSION=mv.version\n",
    "# Set model URI (update with your MLflow model registry path)\n",
    "model_uri = f\"models:/{registered_model_name}/latest\"  # Example for a registry model\n",
    "print(model_uri)\n",
    "# model_uri = \"runs:/your_run_id/model\"  # If stored in a specific run\n",
    "# Load the MLflow model\n",
    "\n",
    "\n",
    "model = mlflow.pyfunc.load_model(model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02978c3-4ca7-46f7-ba6f-2b2ed272c5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.predict({\"prompt\" : [\"Once upon a time in a distant land,\"]})\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
