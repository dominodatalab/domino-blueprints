{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34552e3a-7c6a-40a4-8a4e-20b5ce060010",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch transformers accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013609ab-f9a1-4f17-b4c6-2d685d3efc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f03a88-a71e-4a4f-9656-8fbe48985895",
   "metadata": {},
   "source": [
    "#### Login to Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91dc664-c327-4f9b-aeed-3a6319d85f93",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#Login to hugging face\n",
    "\n",
    "# Replace 'your_huggingface_token' with your actual token\n",
    "token=os.environ[\"HF_TOKEN\"]\n",
    "login(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418ceea2-76ba-4d67-a4d0-23c662de2501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the default dataset of the project to download the model. But it can be any dataset you have access to\n",
    "def get_download_dataset_folder(ds_name,model_name):\n",
    "    ds_dir = os.environ['DOMINO_DATASETS_DIR']    \n",
    "    download_ds_dir = f\"{ds_dir}/{ds_name}/{model_name}\"\n",
    "    return download_ds_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8467c5e-b3e2-4ab9-a043-41952b845097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to a dataset\n",
    "def download_model(model_name,save_path):\n",
    "    print(f\"Downloading {model_name} to location {save_path}\")\n",
    "    # Load the model and tokenizer from Hugging Face\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Save them locally\n",
    "    model.save_pretrained(save_path)\n",
    "    tokenizer.save_pretrained(save_path)\n",
    "\n",
    "    print(f\"Model saved locally at {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cb8116-82d1-48bb-b85c-022b486eaaf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ccc0e54-e2ef-44a1-9e35-351b34ec0add",
   "metadata": {},
   "source": [
    "#### Make sure you accept the terms of service for the model you are downloading from HF\n",
    "\n",
    "1. Download the `Mistral-7B-Instruct-v0.3` model\n",
    "2. Download the `google/gemma-2b` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e3373c-2439-41f9-a1c9-5481b5aa8217",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Choose the model (base or instruct)\n",
    "#model_name = \"mistralai/Mistral-7B-Instruct-v0.3\" \n",
    "#save_path = f\"{download_ds_dir}/{model_name}\"  # Set the directory to save the model\n",
    "\n",
    "#download_model(model_name,save_path)\n",
    "\n",
    "model_name = \"google/gemma-2b\"\n",
    "ds_name = \"llmstore\"\n",
    "save_path = get_download_dataset_folder(ds_name,model_name)\n",
    "download_model(model_name,save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d2e710-5601-4f0d-852c-b62f3f4a9996",
   "metadata": {},
   "source": [
    "### Test the downloaded model locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b711df9-f8ac-425f-83ba-a3db507d3464",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/gemma-2b\"\n",
    "ds_name = \"llmstore\"\n",
    "model_path = get_download_dataset_folder(ds_name,model_name)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, \n",
    "                                             torch_dtype=torch.float16, \n",
    "                                             device_map=\"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Create a text-generation pipeline\n",
    "text_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Generate text\n",
    "prompt = \"Once upon a time in a distant land,\"\n",
    "output = text_generator(prompt, max_length=50, do_sample=True)\n",
    "print(output[0]['generated_text'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df1a208-d4ac-437a-8128-a9c5da8be04f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
